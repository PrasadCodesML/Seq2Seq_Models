{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers\n",
        "\n",
        "Was introduced in Attention is all you need\n",
        "\n",
        "Impact of transformers\n",
        "1. Revolution in NLP\n",
        "2. Democratizing AI -> BERT, GPT for transfer learning\n",
        "3. Mutlimodal capability\n",
        "\n",
        "1. Sequence to Sequence Learning with Neural Networks : The problem with Encoder decoder architecture was the context vector was not able to keep all the information and was showing bad results when the sentence length increase\n",
        "\n",
        "2. Nerual Machine Translation by Jointly Learning and Align and Translate -> We used to send attention vector along with the context vector, here the previous problem was solved but it was slow since here LSTMs were used the input was seqential.\n",
        "\n",
        "3. Attention is all you need -> Introduced transformers and introduced parallel inputs instead of sequential due to which this architecture was highly scalable.\n",
        "\n",
        "2000 - 2014 -> RNNs / LSTMs\n",
        "\n",
        "2014 -> Attention\n",
        "\n",
        "2017 -> Transformers\n",
        "\n",
        "2018 -> BERT / GPT (Transfer Learning)\n",
        "\n",
        "2018 - 2020 -> Vision Transformer, Alpha fold\n",
        "\n",
        "2021 -> Gen AI\n",
        "\n",
        "2022 -> Chat GPT / Stable Diffusion\n",
        "\n",
        "### Advantages of Transformers\n",
        "1. Scalability\n",
        "2. Transfer Learning\n",
        "3. Multimodal capability\n",
        "4. Flexible architecture\n",
        "  - Encoder only BERT\n",
        "  - Decoder only GPT\n",
        "5. Ecosystem\n",
        "  - Hugging Face\n",
        "  - Videos / Blogs\n",
        "6. Integrated AI\n",
        "  - GANs + Transformers\n",
        "  - RL + Transformers\n",
        "  - CNN + Transformers\n",
        "\n",
        "Applications\n",
        "1. ChatGPT\n",
        "2. Dall-E\n",
        "3. Alpha Fold\n",
        "4. OpenAI Codex\n",
        "\n",
        "Disadvagtages\n",
        "1. High computational resources (requries GPUs)\n",
        "2. Lots of Data\n",
        "3. High change of overfitting\n",
        "4. Energy consumption\n",
        "5. Interpretability\n",
        "6. Bais -> If data is baised it will be reflected\n"
      ],
      "metadata": {
        "id": "QUfZbCJlcc0J"
      }
    }
  ]
}