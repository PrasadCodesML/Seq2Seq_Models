{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Self Attention\n",
        "\n",
        "The most important requirement for and NLP task is to convert words into numbers also called as vectorization some techniques are : One Hot Encoding, Bag of Words, TfiDF, Word Embeddings\n",
        "\n",
        "Word embeddings have the power to capture the semantic meanings\n",
        "  - Here when we enter word it convert the word into embedding vectors\n",
        "  - Similar words are closer in the vector space\n",
        "  - Each dimension reveal some aspect about the word\n",
        "\n",
        "The problem with \"Average Meaning\"\n",
        "\n",
        "  - The word embedding does not capture meaning it captures the average meaning\n",
        "  - So is in data, Apple is in form of fruit is more then it's taste aspect will be more compared to technology\n",
        "1. The problem is word embeddings are static, they are made once but used more times.\n",
        "2. Ideally you should have (contextual embedding) dynamic embedding instead of static means based on the context the vector of the word (eg. Apple) should change\n",
        "\n",
        "Self Attention is a mechanism which generates smart contextual embedding from word embeddings\n",
        "\n",
        "Consider self attention as a function -> It does some calculation and you get new embeddings these output embeddings are contextual embeddings, which tells which particular word is used in which context"
      ],
      "metadata": {
        "id": "k1oN5uyCjjNA"
      }
    }
  ]
}