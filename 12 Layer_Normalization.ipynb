{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization\n",
        "\n",
        "Normalization in deep learning refers to the process of transforming data or model outputs to have specific statistical properties, typically a mean of zero and a variance of one.\n",
        "\n",
        "### What are you exactly normalizing\n",
        "1. You can normalize the inputs\n",
        "2. You can normalize the activation in hidden layers\n",
        "\n",
        "### Benefits of Normalization\n",
        "1. Improved Training Stability:\n",
        "  - Normalization helps to stabilize and accelerate the training process by reducing the likelihood of extreme values that can cause gradients to explode or vanish.\n",
        "\n",
        "2. Faster Convergence:\n",
        "  - By normalizing inputs or activations, models can converge more quickly because the gradients have more consistent magnitudes. This allows for more stable updates during backpropagation.\n",
        "\n",
        "3. Mitigating Internal Covariate Shift:\n",
        "  - Internal covariate shift refers to the change in the distribution of layer inputs during training. Normalization techniques, like batch normalization, help to reduce this shift, making the training process more robust.\n",
        "\n",
        "4. Regularization Effect:\n",
        "  - Some normalization techniques, like batch normalization, introduce a slight\n",
        "regularizing effect by adding noise to the mini-batches during training. This can help to reduce overfitting\n",
        "\n",
        "### Batch Normalization\n",
        "\n",
        "Batch normalization is a technique in machine learning that improves the training process of deep neural networks by normalizing the input of each layer. It does so by standardizing the intermediate feature values (activations) within a mini-batch to have a mean of zero and a standard deviation of one, which helps stabilize learning and reduces internal covariate shift. This not only speeds up convergence during training but also acts as a form of regularization, often reducing the need for other methods like dropout.\n",
        "\n"
      ],
      "metadata": {
        "id": "7nka6c5cEB0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why we use layer normalization\n",
        "\n",
        "We use layer normalization because batch normalization does not work well with self attention\n",
        "\n",
        "Batch normalization does not work well on sequential data\n"
      ],
      "metadata": {
        "id": "slvs0NdTGiLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer Normalization\n",
        "\n",
        "Layer normalization is a technique used in machine learning to stabilize and accelerate the training of neural networks. Unlike batch normalization, which operates across a mini-batch, layer normalization normalizes the inputs across the features within a single training example. It ensures that the mean and variance of the activations are consistent within each layer, irrespective of the batch size, making it particularly useful for recurrent neural networks and scenarios with variable batch sizes. This normalization improves convergence and can enhance model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "ooYMkEtvItPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Normalization Vs Layer Normalization\n",
        "\n",
        "| **Aspect**              | **Batch Normalization**                                               | **Layer Normalization**                                             |\n",
        "|--------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
        "| **Normalization Scope** | Operates across a mini-batch, normalizing features for all samples.  | Operates within a single sample, normalizing features across layers. |\n",
        "| **Batch Dependency**    | Depends on the batch size and composition.                          | Independent of batch size, works well with a single example.         |\n",
        "| **Best Suited For**     | Effective for feedforward and convolutional networks.               | Ideal for recurrent neural networks (RNNs) or variable batch sizes.  |\n",
        "| **Behavior**            | Can vary with batch composition, leading to instability in small batches. | Consistent behavior regardless of batch size.                       |\n"
      ],
      "metadata": {
        "id": "-00hkTyfJVrd"
      }
    }
  ]
}